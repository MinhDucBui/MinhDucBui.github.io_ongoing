---
---
References
==========


@inproceedings{torgo_ribeiro,
author = {Torgo, Luís and Ribeiro, Rita},
year = {2007},
month = {09},
pages = {597-604},
title = {Utility-Based Regression},
isbn = {978-3-540-74975-2},
doi = {10.1007/978-3-540-74976-9_63}
}

@misc{stanton2021does,
      title={Does Knowledge Distillation Really Work?}, 
      author={Samuel Stanton and Pavel Izmailov and Polina Kirichenko and Alexander A. Alemi and Andrew Gordon Wilson},
      year={2021},
      eprint={2106.05945},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mobahi_2020,
  doi = {10.48550/ARXIV.2002.05715},
  
  url = {https://arxiv.org/abs/2002.05715},
  
  author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L.},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Self-Distillation Amplifies Regularization in Hilbert Space},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{furlanello_2018,
  doi = {10.48550/ARXIV.1805.04770},
  
  url = {https://arxiv.org/abs/1805.04770},
  
  author = {Furlanello, Tommaso and Lipton, Zachary C. and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  
  keywords = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Born Again Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Jelinek1977PerplexityaMO,
  title={Perplexity—a measure of the difficulty of speech recognition tasks},
  author={Frederick Jelinek and Robert L. Mercer and Lalit R. Bahl and J. Baker},
  journal={Journal of the Acoustical Society of America},
  year={1977},
  volume={62}
}

@inproceedings{Conneau2018XNLIEC,
  title={XNLI: Evaluating Cross-lingual Sentence Representations},
  author={Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},
  booktitle={EMNLP},
  year={2018}
}


@misc{ansell_2021,
  doi = {10.48550/ARXIV.2110.07560},
  
  url = {https://arxiv.org/abs/2110.07560},
  
  author = {Ansell, Alan and Ponti, Edoardo Maria and Korhonen, Anna and Vulić, Ivan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{guo_2020,
  doi = {10.48550/ARXIV.2012.07463},
  url = {https://arxiv.org/abs/2012.07463},
  author = {Guo, Demi and Rush, Alexander M. and Kim, Yoon},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Parameter-Efficient Transfer Learning with Diff Pruning},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{frankle_2018,
  doi = {10.48550/ARXIV.1803.03635},
  url = {https://arxiv.org/abs/1803.03635},
  author = {Frankle, Jonathan and Carbin, Michael},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{bitfit_2021,
  doi = {10.48550/ARXIV.2106.10199},
  
  url = {https://arxiv.org/abs/2106.10199},
  
  author = {Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}




@misc{williams_2017,
  doi = {10.48550/ARXIV.1704.05426},
  url = {https://arxiv.org/abs/1704.05426},
  
  author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{roemmele_2011,
author = {Roemmele, Melissa and Bejan, Cosmin and Gordon, Andrew},
year = {2011},
month = {01},
pages = {},
title = {Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
journal = {AAAI Spring Symposium - Technical Report}
}

@misc{rashimi_2019,
  doi = {10.48550/ARXIV.1902.00193},
  
  url = {https://arxiv.org/abs/1902.00193},
  
  author = {Rahimi, Afshin and Li, Yuan and Cohn, Trevor},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Massively Multilingual Transfer for NER},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{ponti_2020,
  doi = {10.48550/ARXIV.2005.00333},
  
  url = {https://arxiv.org/abs/2005.00333},
  
  author = {Ponti, Edoardo Maria and Glavaš, Goran and Majewska, Olga and Liu, Qianchu and Vulić, Ivan and Korhonen, Anna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Hao_2019,
  doi = {10.48550/ARXIV.1908.05620},
  url = {https://arxiv.org/abs/1908.05620},
  author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Visualizing and Understanding the Effectiveness of BERT},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and Rich Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD '06},
  year={2006}
}

@inproceedings{collobert_2008,
author = {Collobert, Ronan and Weston, Jason},
title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}


@article{doi:10.1080/00437956.1954.11659520,
author = {Zellig S. Harris},
title = {Distributional Structure},
journal = {<i>WORD</i>},
volume = {10},
number = {2-3},
pages = {146-162},
year  = {1954},
publisher = {Routledge},
doi = {10.1080/00437956.1954.11659520},

URL = { 
        https://doi.org/10.1080/00437956.1954.11659520
    
},
eprint = { 
        https://doi.org/10.1080/00437956.1954.11659520
    
}

}


@inproceedings{10.1145/2766462.2767752,
author = {Vuli\'{c}, Ivan and Moens, Marie-Francine},
title = {Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings},
year = {2015},
isbn = {9781450336215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2766462.2767752},
doi = {10.1145/2766462.2767752},
abstract = {We propose a new unified framework for monolingual (MoIR) and cross-lingual information retrieval (CLIR) which relies on the induction of dense real-valued word vectors known as word embeddings (WE) from comparable data. To this end, we make several important contributions: (1) We present a novel word representation learning model called Bilingual Word Embeddings Skip-Gram (BWESG) which is the first model able to learn bilingual word embeddings solely on the basis of document-aligned comparable data; (2) We demonstrate a simple yet effective approach to building document embeddings from single word embeddings by utilizing models from compositional distributional semantics. BWESG induces a shared cross-lingual embedding vector space in which both words, queries, and documents may be presented as dense real-valued vectors; (3) We build novel ad-hoc MoIR and CLIR models which rely on the induced word and document embeddings and the shared cross-lingual embedding space; (4) Experiments for English and Dutch MoIR, as well as for English-to-Dutch and Dutch-to-English CLIR using benchmarking CLEF 2001-2003 collections and queries demonstrate the utility of our WE-based MoIR and CLIR models. The best results on the CLEF collections are obtained by the combination of the WE-based approach and a unigram language model. We also report on significant improvements in ad-hoc IR tasks of our WE-based framework over the state-of-the-art framework for learning text representations from comparable data based on latent Dirichlet allocation (LDA).},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {363–372},
numpages = {10},
keywords = {cross-lingual information retrieval, comparable data, word embeddings, vector space retrieval models, ad-hoc retrieval, text representation learning, semantic composition, multilinguality},
location = {Santiago, Chile},
series = {SIGIR '15}
}

@inproceedings{lample-etal-2018-phrase,
    title = "Phrase-Based {\&} Neural Unsupervised Machine Translation",
    author = "Lample, Guillaume  and
      Ott, Myle  and
      Conneau, Alexis  and
      Denoyer, Ludovic  and
      Ranzato, Marc{'}Aurelio",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1549",
    doi = "10.18653/v1/D18-1549",
    pages = "5039--5049",
    abstract = "Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT{'}14 English-French and WMT{'}16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.",
}

@misc{artexe_mt,
  doi = {10.48550/ARXIV.1710.11041},
  
  url = {https://arxiv.org/abs/1710.11041},
  
  author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Neural Machine Translation},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{qi-etal-2018-pre,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2084",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
    abstract = "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases {--} providing gains of up to 20 BLEU points in the most favorable setting.",
}


@misc{adapterhub_2020,
  doi = {10.48550/ARXIV.2007.07779},
  
  url = {https://arxiv.org/abs/2007.07779},
  
  author = {Pfeiffer, Jonas and Rücklé, Andreas and Poth, Clifton and Kamath, Aishwarya and Vulić, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AdapterHub: A Framework for Adapting Transformers},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{yang2021knowledge, 
  title={Knowledge distillation via softmax regression representation learning},
  author={Jing Yang and Brais Martinez and Adrian Bulat and Georgios Tzimiropoulos},
  booktitle={ICLR2021},
  year={2021}  
}
@misc{bapna2019simple,
      title={Simple, Scalable Adaptation for Neural Machine Translation}, 
      author={Ankur Bapna and Naveen Arivazhagan and Orhan Firat},
      year={2019},
      eprint={1909.08478},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pfeiffer2021adapterfusion,
      title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning}, 
      author={Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},
      year={2021},
      eprint={2005.00247},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{stickland2019bert,
      title={BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning}, 
      author={Asa Cooper Stickland and Iain Murray},
      year={2019},
      eprint={1902.02671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}




@misc{ba_deep_2013,
  doi = {10.48550/ARXIV.1312.6184},
  
  url = {https://arxiv.org/abs/1312.6184},
  
  author = {Ba, Lei Jimmy and Caruana, Rich},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Do Deep Nets Really Need to be Deep?},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{Gou_2021,
	doi = {10.1007/s11263-021-01453-z},
  
	url = {https://doi.org/10.1007%2Fs11263-021-01453-z},
  
	year = 2021,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {129},
  
	number = {6},
  
	pages = {1789--1819},
  
	author = {Jianping Gou and Baosheng Yu and Stephen J. Maybank and Dacheng Tao},
  
	title = {Knowledge Distillation: A Survey},
  
	journal = {International Journal of Computer Vision}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{Krizhevsky_2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
journal = {Neural Information Processing Systems},
doi = {10.1145/3065386}
}


@misc{louizos_2017,
  doi = {10.48550/ARXIV.1712.01312},
  
  url = {https://arxiv.org/abs/1712.01312},
  
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Sparse Neural Networks through $L_0$ Regularization},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{sbert_reimers_2019,
  doi = {10.48550/ARXIV.1908.10084},
  
  url = {https://arxiv.org/abs/1908.10084},
  
  author = {Reimers, Nils and Gurevych, Iryna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@inproceedings{reimer_sbert,
author = {Reimers, Nils and Gurevych, Iryna},
year = {2020},
month = {01},
pages = {4512-4525},
title = {Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation},
doi = {10.18653/v1/2020.emnlp-main.365}
}

@misc{houlsby2019parameterefficient,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rebuffi2017learning,
      title={Learning multiple visual domains with residual adapters}, 
      author={Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
      year={2017},
      eprint={1705.08045},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{TIEDEMANN12.463,
  author = {Jörg Tiedemann},
  title = {Parallel Data, Tools and Interfaces in OPUS},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
 }

@inproceedings{wenzek-etal-2020-ccnet,
    title = "{CCN}et: Extracting High Quality Monolingual Datasets from Web Crawl Data",
    author = "Wenzek, Guillaume  and
      Lachaux, Marie-Anne  and
      Conneau, Alexis  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Joulin, Armand  and
      Grave, Edouard",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.494",
    pages = "4003--4012",
    abstract = "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@misc{clark2019does,
      title={What Does BERT Look At? An Analysis of BERT's Attention}, 
      author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
      year={2019},
      eprint={1906.04341},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiao2020tinybert,
      title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
      author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
      year={2020},
      eprint={1909.10351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{romero2015fitnets,
      title={FitNets: Hints for Thin Deep Nets}, 
      author={Adriana Romero and Nicolas Ballas and Samira Ebrahimi Kahou and Antoine Chassang and Carlo Gatta and Yoshua Bengio},
      year={2015},
      eprint={1412.6550},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cao2020multilingual,
      title={Multilingual Alignment of Contextual Word Representations}, 
      author={Steven Cao and Nikita Kitaev and Dan Klein},
      year={2020},
      eprint={2002.03518},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{pires-etal-2019-multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
}

@misc{hu2020xtreme,
      title={XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization}, 
      author={Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
      year={2020},
      eprint={2003.11080},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mikolov2013exploiting,
      title={Exploiting Similarities among Languages for Machine Translation}, 
      author={Tomas Mikolov and Quoc V. Le and Ilya Sutskever},
      year={2013},
      eprint={1309.4168},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hermann-blunsom-2014-multilingual,
    title = "Multilingual Models for Compositional Distributed Semantics",
    author = "Hermann, Karl Moritz  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1006",
    doi = "10.3115/v1/P14-1006",
    pages = "58--68",
}

@misc{peters2018deep,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{howard2018universal,
      title={Universal Language Model Fine-tuning for Text Classification}, 
      author={Jeremy Howard and Sebastian Ruder},
      year={2018},
      eprint={1801.06146},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{10.1162/089120102762671990,
    author = {Plath, Warren J.},
    title = "{Early Years in Machine Translation: Memoirs and Biographies of Pioneers}",
    journal = {Computational Linguistics},
    volume = {28},
    number = {4},
    pages = {554-559},
    year = {2002},
    month = {12},
    issn = {0891-2017},
    doi = {10.1162/089120102762671990},
    url = {https://doi.org/10.1162/089120102762671990},
    eprint = {https://direct.mit.edu/coli/article-pdf/28/4/554/1797894/089120102762671990.pdf},
}

@misc{mukherjee2020xtremedistil,
      title={XtremeDistil: Multi-stage Distillation for Massive Multilingual Models}, 
      author={Subhabrata Mukherjee and Ahmed Awadallah},
      year={2020},
      eprint={2004.05686},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{aguilar2020knowledge,
      title={Knowledge Distillation from Internal Representations}, 
      author={Gustavo Aguilar and Yuan Ling and Yu Zhang and Benjamin Yao and Xing Fan and Chenlei Guo},
      year={2020},
      eprint={1910.03723},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2019patient,
      title={Patient Knowledge Distillation for BERT Model Compression}, 
      author={Siqi Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
      year={2019},
      eprint={1908.09355},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{dufter2021identifying,
      title={Identifying Necessary Elements for BERT's Multilinguality}, 
      author={Philipp Dufter and Hinrich Schütze},
      year={2021},
      eprint={2005.00396},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{noauthororeditor,
  added-at = {2019-04-23T14:03:30.000+0200},
  author = {Ruder, Sebastian},
  biburl = {https://www.bibsonomy.org/bibtex/2a618e15e6a2195880690ccdf3a4da53f/kirk86},
  description = {Neural Transfer Learning for Natural Language Processing},
  interhash = {d0f4e9317f2278f079ff164a2752e2d5},
  intrahash = {a618e15e6a2195880690ccdf3a4da53f},
  keywords = {deep-learning nlp thesis},
  timestamp = {2019-04-23T14:03:30.000+0200},
  title = {Neural Transfer Learning for Natural Language Processing},
  url = {http://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf},
  year = 2019
}

@inproceedings{Goddard2006NaturalSM,
  title={Natural Semantic Metalanguage},
  author={Cliff Goddard},
  year={2006}
}

@inproceedings{ruder2019unsupervised,
  title={Unsupervised Cross-Lingual Representation Learning},
  author={Ruder, Sebastian and S{\o}gaard, Anders and Vuli{\'c}, Ivan}, 
  booktitle={Proceedings of ACL 2019, Tutorial Abstracts},
  pages={31--38},
  year={2019}
}



@article{ruder_2019,
   title={A Survey of Cross-lingual Word Embedding Models},
   volume={65},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.11640},
   DOI={10.1613/jair.1.11640},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
   year={2019},
   month={Aug},
   pages={569–631}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2020unsupervised,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{artetxe-etal-2020-call,
    title = "A Call for More Rigor in Unsupervised Cross-lingual Learning",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.658",
    doi = "10.18653/v1/2020.acl-main.658",
    pages = "7375--7388",
    abstract = "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world{'}s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",
}

@inproceedings{chung-etal-2020-improving,
    title = "Improving Multilingual Models with Language-Clustered Vocabularies",
    author = "Chung, Hyung Won  and
      Garrette, Dan  and
      Tan, Kiat Chuan  and
      Riesa, Jason",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.367",
    doi = "10.18653/v1/2020.emnlp-main.367",
    pages = "4536--4546",
    abstract = "State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1{\%}), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",
}


@misc{virtanen2019multilingual,
      title={Multilingual is not enough: BERT for Finnish}, 
      author={Antti Virtanen and Jenna Kanerva and Rami Ilo and Jouni Luoma and Juhani Luotolahti and Tapio Salakoski and Filip Ginter and Sampo Pyysalo},
      year={2019},
      eprint={1912.07076},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{antoun-etal-2020-arabert,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://aclanthology.org/2020.osact-1.2",
    pages = "9--15",
    abstract = "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
    language = "English",
    ISBN = "979-10-95546-51-1",
}

@inproceedings{chen-etal-2019-multi-source,
    title = "Multi-Source Cross-Lingual Model Transfer: Learning What to Share",
    author = "Chen, Xilun  and
      Awadallah, Ahmed Hassan  and
      Hassan, Hany  and
      Wang, Wei  and
      Cardie, Claire",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1299",
    doi = "10.18653/v1/P19-1299",
    pages = "3098--3112",
    abstract = "Modern NLP applications have enjoyed a great boost utilizing neural networks models. Such deep neural models, however, are not applicable to most human languages due to the lack of annotated training data for various NLP tasks. Cross-lingual transfer learning (CLTL) is a viable method for building NLP models for a low-resource target language by leveraging labeled data from other (source) languages. In this work, we focus on the multilingual transfer setting where training data in multiple source languages is leveraged to further boost target language performance. Unlike most existing methods that rely only on language-invariant features for CLTL, our approach coherently utilizes both language-invariant and language-specific features at instance level. Our model leverages adversarial networks to learn language-invariant features, and mixture-of-experts models to dynamically exploit the similarity between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. Moreover, when coupled with unsupervised multilingual embeddings, our model can operate in a zero-resource setting where neither target language training data nor cross-lingual resources are available. Our model achieves significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale industry dataset.",
}

@misc{xie2018neural,
      title={Neural Cross-Lingual Named Entity Recognition with Minimal Resources}, 
      author={Jiateng Xie and Zhilin Yang and Graham Neubig and Noah A. Smith and Jaime Carbonell},
      year={2018},
      eprint={1808.09861},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{kim-etal-2017-cross,
    title = "Cross-Lingual Transfer Learning for {POS} Tagging without Cross-Lingual Resources",
    author = "Kim, Joo-Kyung  and
      Kim, Young-Bum  and
      Sarikaya, Ruhi  and
      Fosler-Lussier, Eric",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1302",
    doi = "10.18653/v1/D17-1302",
    pages = "2832--2838",
    abstract = "Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language. In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora. The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations. The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language. Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",
}


@InProceedings{pmlr-v37-ganin15,
  title = 	 {Unsupervised Domain Adaptation by Backpropagation},
  author = 	 {Ganin, Yaroslav and Lempitsky, Victor},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1180--1189},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ganin15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ganin15.html},
  abstract = 	 {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}


@article{JMLR:v17:15-239,
  author  = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran{\c{c}}ois Laviolette and Mario March and Victor Lempitsky},
  title   = {Domain-Adversarial Training of Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {59},
  pages   = {1-35},
  url     = {http://jmlr.org/papers/v17/15-239.html}
}

@misc{henderson2017efficient,
      title={Efficient Natural Language Response Suggestion for Smart Reply}, 
      author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},
      year={2017},
      eprint={1705.00652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wei2021learning,
      title={On Learning Universal Representations Across Languages}, 
      author={Xiangpeng Wei and Rongxiang Weng and Yue Hu and Luxi Xing and Heng Yu and Weihua Luo},
      year={2021},
      eprint={2007.15960},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{10.1145/3097983.3098202,
author = {Chen, Ting and Sun, Yizhou and Shi, Yue and Hong, Liangjie},
title = {On Sampling Strategies for Neural Network-Based Collaborative Filtering},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098202},
doi = {10.1145/3097983.3098202},
abstract = {Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing state-of-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the user-item interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions "graph-based" loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to $times 30$ times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graph-based loss functions, and would also enable more research under the neural network-based recommendation framework.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {767–776},
numpages = {10},
keywords = {sampling strategies, collaborative filtering, neural networks},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@INPROCEEDINGS{1640964,
  author={Hadsell, R. and Chopra, S. and LeCun, Y.},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
  title={Dimensionality Reduction by Learning an Invariant Mapping}, 
  year={2006},
  volume={2},
  number={},
  pages={1735-1742},
  doi={10.1109/CVPR.2006.100}}

@misc{chen2020simple,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      eprint={2002.05709},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mad_x,
  doi = {10.48550/ARXIV.2005.00052},
  
  url = {https://arxiv.org/abs/2005.00052},
  
  author = {Pfeiffer, Jonas and Vulić, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{radford2017learning,
      title={Learning to Generate Reviews and Discovering Sentiment}, 
      author={Alec Radford and Rafal Jozefowicz and Ilya Sutskever},
      year={2017},
      eprint={1704.01444},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kiros2015skipthought,
      title={Skip-Thought Vectors}, 
      author={Ryan Kiros and Yukun Zhu and Ruslan Salakhutdinov and Richard S. Zemel and Antonio Torralba and Raquel Urtasun and Sanja Fidler},
      year={2015},
      eprint={1506.06726},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Schroff_2015,
   title={FaceNet: A unified embedding for face recognition and clustering},
   url={http://dx.doi.org/10.1109/CVPR.2015.7298682},
   DOI={10.1109/cvpr.2015.7298682},
   journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
   year={2015},
   month={Jun}
}


@article{10.5555/1577069.1577078,
author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {207–244},
numpages = {38}
}

@INPROCEEDINGS{1467314,
  author={Chopra, S. and Hadsell, R. and LeCun, Y.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Learning a similarity metric discriminatively, with application to face verification}, 
  year={2005},
  volume={1},
  number={},
  pages={539-546 vol. 1},
  doi={10.1109/CVPR.2005.202}}

@misc{arora2019theoretical,
      title={A Theoretical Analysis of Contrastive Unsupervised Representation Learning}, 
      author={Sanjeev Arora and Hrishikesh Khandeparkar and Mikhail Khodak and Orestis Plevrakis and Nikunj Saunshi},
      year={2019},
      eprint={1902.09229},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2019unicoder,
      title={Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks}, 
      author={Haoyang Huang and Yaobo Liang and Nan Duan and Ming Gong and Linjun Shou and Daxin Jiang and Ming Zhou},
      year={2019},
      eprint={1909.00964},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
    abstract = "We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: {``}Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?{''} A: {``}Make sure no one else could hear{''}). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textgreater}20{\%} gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).",
}

@article{rabinovich-etal-2018-native,
    title = "Native Language Cognate Effects on Second Language Lexical Choice",
    author = "Rabinovich, Ella  and
      Tsvetkov, Yulia  and
      Wintner, Shuly",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1024",
    doi = "10.1162/tacl_a_00024",
    pages = "329--342",
    abstract = "We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the phylogenetic language tree of the Indo-European language family solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.",
}

@inproceedings{tatman-2017-gender,
    title = "Gender and Dialect Bias in {Y}ou{T}ube{'}s Automatic Captions",
    author = "Tatman, Rachael",
    booktitle = "Proceedings of the First {ACL} Workshop on Ethics in Natural Language Processing",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1606",
    doi = "10.18653/v1/W17-1606",
    pages = "53--59",
    abstract = "This project evaluates the accuracy of YouTube{'}s automatically-generated captions across two genders and five dialect groups. Speakers{'} dialect and gender was controlled for by using videos uploaded as part of the {``}accent tag challenge{''}, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker{'}s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.",
}

@inproceedings{Zhiltsova2019MitigationOU,
  title={Mitigation of Unintended Biases against Non-Native English Texts in Sentiment Analysis},
  author={Alina Zhiltsova and Simon Caton and Catherine Mulway},
  booktitle={AICS},
  year={2019}
}

@inproceedings{pappas-etal-2018-beyond,
    title = "Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation",
    author = "Pappas, Nikolaos  and
      Miculicich, Lesly  and
      Henderson, James",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6308",
    doi = "10.18653/v1/W18-6308",
    pages = "73--83",
    abstract = "Tying the weights of the target word embeddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality. Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters. In particular, we propose a \textit{structure-aware} output layer which captures the semantic structure of the output space of words within a joint input-output embedding. The model is a generalized form of \textit{weight tying} which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled. In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them. Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder baselines trained with or without \textit{weight tying}.",
}

@misc{weight_tying_2016,
  doi = {10.48550/ARXIV.1608.05859},
  
  url = {https://arxiv.org/abs/1608.05859},
  
  author = {Press, Ofir and Wolf, Lior},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Using the Output Embedding to Improve Language Models},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{subword_unigram_2018,
  doi = {10.48550/ARXIV.1804.10959},
  
  url = {https://arxiv.org/abs/1804.10959},
  
  author = {Kudo, Taku},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{sentencepiece_2018,
  doi = {10.48550/ARXIV.1808.06226},
  
  url = {https://arxiv.org/abs/1808.06226},
  
  author = {Kudo, Taku and Richardson, John},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2020xlnet,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
      author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{joshi2020spanbert,
      title={SpanBERT: Improving Pre-training by Representing and Predicting Spans}, 
      author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
      year={2020},
      eprint={1907.10529},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{dong2019robust,
  title={A robust self-learning framework for cross-lingual text classification},
  author={Dong, Xin Luna and de Melo, Gerard},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={6306--6310},
  year={2019}
}

@misc{wang2021multiadversarial,
      title={Multi-Adversarial Learning for Cross-Lingual Word Embeddings}, 
      author={Haozhou Wang and James Henderson and Paola Merlo},
      year={2021},
      eprint={2010.08432},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{torregrossa_2021,
author = {Torregrossa, François and Robin, Allesiardo and Claveau, Vincent and Kooli, Nihel and Gravier, Guillaume},
year = {2021},
month = {03},
pages = {1-19},
title = {A survey on training and evaluation of word embeddings},
volume = {11},
journal = {International Journal of Data Science and Analytics},
doi = {10.1007/s41060-021-00242-8}
}

@inproceedings{luong-etal-2015-bilingual,
    title = "Bilingual Word Representations with Monolingual Quality in Mind",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-1521",
    doi = "10.3115/v1/W15-1521",
    pages = "151--159",
}

@inproceedings{klementiev-etal-2012-inducing,
    title = "Inducing Crosslingual Distributed Representations of Words",
    author = "Klementiev, Alexandre  and
      Titov, Ivan  and
      Bhattarai, Binod",
    booktitle = "Proceedings of {COLING} 2012",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://aclanthology.org/C12-1089",
    pages = "1459--1474",
}


@misc{vulvic_2019_bli,
  doi = {10.48550/ARXIV.1909.01638},
  
  url = {https://arxiv.org/abs/1909.01638},
  
  author = {Vulić, Ivan and Glavaš, Goran and Reichart, Roi and Korhonen, Anna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Do We Really Need Fully Unsupervised Cross-Lingual Embeddings?},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{alvarez-melis-jaakkola-2018-gromov,
    title = "{G}romov-{W}asserstein Alignment of Word Embedding Spaces",
    author = "Alvarez-Melis, David  and
      Jaakkola, Tommi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1214",
    doi = "10.18653/v1/D18-1214",
    pages = "1881--1890",
    abstract = "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",
}


@inproceedings{hoshen-wolf-2018-non,
    title = "Non-Adversarial Unsupervised Word Translation",
    author = "Hoshen, Yedid  and
      Wolf, Lior",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1043",
    doi = "10.18653/v1/D18-1043",
    pages = "469--478",
    abstract = "Unsupervised word translation from non-parallel inter-lingual corpora has attracted much research interest. Very recently, neural network methods trained with adversarial loss functions achieved high accuracy on this task. Despite the impressive success of the recent techniques, they suffer from the typical drawbacks of generative adversarial models: sensitivity to hyper-parameters, long training time and lack of interpretability. In this paper, we make the observation that two sufficiently similar distributions can be aligned correctly with iterative matching methods. We present a novel method that first aligns the second moment of the word distributions of the two languages and then iteratively refines the alignment. Extensive experiments on word translation of European and Non-European languages show that our method achieves better performance than recent state-of-the-art deep adversarial approaches and is competitive with the supervised baseline. It is also efficient, easy to parallelize on CPU and interpretable.",
}

@inproceedings{artetxe-etal-2018-robust,
    title = "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1073",
    doi = "10.18653/v1/P18-1073",
    pages = "789--798",
    abstract = "Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at \url{https://github.com/artetxem/vecmap}.",
}

@inproceedings{huang-etal-2015-translation,
    title = "Translation Invariant Word Embeddings",
    author = "Huang, Kejun  and
      Gardner, Matt  and
      Papalexakis, Evangelos  and
      Faloutsos, Christos  and
      Sidiropoulos, Nikos  and
      Mitchell, Tom  and
      Talukdar, Partha P.  and
      Fu, Xiao",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1127",
    doi = "10.18653/v1/D15-1127",
    pages = "1084--1088",
}

@ARTICLE{RePEc:spr:psycho:v:31:y:1966:i:1:p:1-10,
title = {A generalized solution of the orthogonal procrustes problem},
author = {Schönemann, Peter},
year = {1966},
journal = {Psychometrika},
volume = {31},
number = {1},
pages = {1-10},
url = {https://EconPapers.repec.org/RePEc:spr:psycho:v:31:y:1966:i:1:p:1-10}
}

@inproceedings{xing-etal-2015-normalized,
    title = "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation",
    author = "Xing, Chao  and
      Wang, Dong  and
      Liu, Chao  and
      Lin, Yiye",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N15-1104",
    doi = "10.3115/v1/N15-1104",
    pages = "1006--1011",
}

@article{Galassi_2021,
	doi = {10.1109/tnnls.2020.3019893},
  
	url = {https://doi.org/10.1109%2Ftnnls.2020.3019893},
  
	year = 2021,
	month = {oct},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {32},
  
	number = {10},
  
	pages = {4291--4308},
  
	author = {Andrea Galassi and Marco Lippi and Paolo Torroni},
  
	title = {Attention in Natural Language Processing},
  
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}


@misc{attention_2014,
  doi = {10.48550/ARXIV.1409.0473},
  
  url = {https://arxiv.org/abs/1409.0473},
  
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Kalchbrenner2013RecurrentCT,
  title={Recurrent Continuous Translation Models},
  author={Nal Kalchbrenner and Phil Blunsom},
  booktitle={EMNLP},
  year={2013}
}


@inproceedings{Hinton1993AutoencodersMD,
  title={Autoencoders, Minimum Description Length and Helmholtz Free Energy},
  author={Geoffrey E. Hinton and Richard S. Zemel},
  booktitle={NIPS},
  year={1993}
}

@inproceedings{Sutskever2014SequenceTS,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle={NIPS},
  year={2014}
}

@inproceedings{mikolov_rnn_2011,
author = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, J.H. and Khudanpur, Sanjeev},
year = {2011},
month = {06},
pages = {5528 - 5531},
title = {Extensions of recurrent neural network language model},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2011.5947611}
}

@inproceedings{sutskever_2011,
author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey},
year = {2011},
month = {01},
pages = {1017-1024},
title = {Generating Text with Recurrent Neural Networks},
journal = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)}
}

@inproceedings{Kusner2015FromWE,
  title={From Word Embeddings To Document Distances},
  author={Matt J. Kusner and Yu Sun and Nicholas I. Kolkin and Kilian Q. Weinberger},
  booktitle={ICML},
  year={2015}
}

@misc{kiros_2015,
  doi = {10.48550/ARXIV.1506.06726},
  
  url = {https://arxiv.org/abs/1506.06726},
  
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Skip-Thought Vectors},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lample_2016,
  doi = {10.48550/ARXIV.1603.01360},
  
  url = {https://arxiv.org/abs/1603.01360},
  
  author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Architectures for Named Entity Recognition},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{gers_2000,
author = {Gers, Felix and Schmidhuber, Jürgen and Cummins, Fred},
year = {2000},
month = {10},
pages = {2451-71},
title = {Learning to Forget: Continual Prediction with LSTM},
volume = {12},
journal = {Neural computation},
doi = {10.1162/089976600300015015}
}

@article{lstm_1997,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}

@article{Le2015ASW,
  title={A Simple Way to Initialize Recurrent Networks of Rectified Linear Units},
  author={Quoc V. Le and Navdeep Jaitly and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2015},
  volume={abs/1504.00941}
}

@misc{relu_2018,
  doi = {10.48550/ARXIV.1803.08375},
  
  url = {https://arxiv.org/abs/1803.08375},
  
  author = {Agarap, Abien Fred},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Learning using Rectified Linear Units (ReLU)},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@misc{pascanu_2021,
  doi = {10.48550/ARXIV.1211.5063},
  
  url = {https://arxiv.org/abs/1211.5063},
  
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the difficulty of training Recurrent Neural Networks},
  
  publisher = {arXiv},
  
  year = {2012},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Bengio1994LearningLD,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Yoshua Bengio and Patrice Y. Simard and Paolo Frasconi},
  journal={IEEE transactions on neural networks},
  year={1994},
  volume={5 2},
  pages={
          157-66
        }
}

@article{vanishing_1998,
author = {Hochreiter, Sepp},
year = {1998},
month = {04},
pages = {107-116},
title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
volume = {6},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
doi = {10.1142/S0218488598000094}
}

@misc{fasttext_2016,
  doi = {10.48550/ARXIV.1607.04606},
  
  url = {https://arxiv.org/abs/1607.04606},
  
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Enriching Word Vectors with Subword Information},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{wu_emerging_2019,
  doi = {10.48550/ARXIV.1911.01464},
  
  url = {https://arxiv.org/abs/1911.01464},
  
  author = {Wu, Shijie and Conneau, Alexis and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Emerging Cross-lingual Structure in Pretrained Language Models},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{smith-etal-2016-well,
    title = "Does {`}well-being{'} translate on {T}witter?",
    author = "Smith, Laura  and
      Giorgi, Salvatore  and
      Solanki, Rishi  and
      Eichstaedt, Johannes  and
      Schwartz, H. Andrew  and
      Abdul-Mageed, Muhammad  and
      Buffone, Anneke  and
      Ungar, Lyle",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1217",
    doi = "10.18653/v1/D16-1217",
    pages = "2042--2047",
}

@article{mohammad_2016,
author = {Mohammad, Saif and Salameh, Mohammad and Kiritchenko, Svetlana},
year = {2016},
month = {01},
pages = {95-130},
title = {How Translation Alters Sentiment},
volume = {55},
journal = {J. Artif. Intell. Res. (JAIR)},
doi = {10.1613/jair.4787}
}

@misc{translation_conneau_2017,
  doi = {10.48550/ARXIV.1710.04087},
  
  url = {https://arxiv.org/abs/1710.04087},
  
  author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and Jégou, Hervé},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Word Translation Without Parallel Data},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{guzman_2019,
author = {Guzman, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc’Aurelio},
year = {2019},
month = {01},
pages = {6100-6113},
title = {The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali–English and Sinhala–English},
doi = {10.18653/v1/D19-1632}
}

@misc{wordpiece_2016,
  doi = {10.48550/ARXIV.1609.08144},
  
  url = {https://arxiv.org/abs/1609.08144},
  
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bpe_2015,
  doi = {10.48550/ARXIV.1508.07909},
  
  url = {https://arxiv.org/abs/1508.07909},
  
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Machine Translation of Rare Words with Subword Units},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{smith2017offline,
      title={Offline bilingual word vectors, orthogonal transformations and the inverted softmax}, 
      author={Samuel L. Smith and David H. P. Turban and Steven Hamblin and Nils Y. Hammerla},
      year={2017},
      eprint={1702.03859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{artetxe-etal-2016-learning,
    title = "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1250",
    doi = "10.18653/v1/D16-1250",
    pages = "2289--2294",
}

@inproceedings{faruqui-dyer-2014-improving,
    title = "Improving Vector Space Word Representations Using Multilingual Correlation",
    author = "Faruqui, Manaal  and
      Dyer, Chris",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E14-1049",
    doi = "10.3115/v1/E14-1049",
    pages = "462--471",
}

@misc{chen2018adversarial,
      title={Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification}, 
      author={Xilun Chen and Yu Sun and Ben Athiwaratkun and Claire Cardie and Kilian Weinberger},
      year={2016},
      eprint={1606.01614},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{keung2020adversarial,
      title={Adversarial Learning with Contextual Embeddings for Zero-resource Cross-lingual Classification and NER}, 
      author={Phillip Keung and Yichao Lu and Vikas Bhardwaj},
      year={2020},
      eprint={1909.00153},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pan-etal-2017-cross,
    title = "Cross-lingual Name Tagging and Linking for 282 Languages",
    author = "Pan, Xiaoman  and
      Zhang, Boliang  and
      May, Jonathan  and
      Nothman, Joel  and
      Knight, Kevin  and
      Ji, Heng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1178",
    doi = "10.18653/v1/P17-1178",
    pages = "1946--1958",
    abstract = "The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{Artetxe_2019,
	doi = {10.1162/tacl_a_00288},
  
	url = {https://doi.org/10.1162%2Ftacl_a_00288},
  
	year = 2019,
	month = {nov},
  
	publisher = {{MIT} Press - Journals},
  
	volume = {7},
  
	pages = {597--610},
  
	author = {Mikel Artetxe and Holger Schwenk},
  
	title = {Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
  
	journal = {Transactions of the Association for Computational Linguistics}
}

@inproceedings{zhang-etal-2017-adversarial,
    title = "Adversarial Training for Unsupervised Bilingual Lexicon Induction",
    author = "Zhang, Meng  and
      Liu, Yang  and
      Luan, Huanbo  and
      Sun, Maosong",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1179",
    doi = "10.18653/v1/P17-1179",
    pages = "1959--1970",
    abstract = "Word embeddings are well known to capture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon. In this work, we show that such cross-lingual connection can actually be established without any form of supervision. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues.",
}

@misc{liu2019improving,
      title={Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding}, 
      author={Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao},
      year={2019},
      eprint={1904.09482},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tan2019multilingual,
      title={Multilingual Neural Machine Translation with Knowledge Distillation}, 
      author={Xu Tan and Yi Ren and Di He and Tao Qin and Zhou Zhao and Tie-Yan Liu},
      year={2019},
      eprint={1902.10461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{turc2019wellread,
      title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models}, 
      author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1908.08962},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2020mkd,
      title={MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models}, 
      author={Linqing Liu and Huan Wang and Jimmy Lin and Richard Socher and Caiming Xiong},
      year={2020},
      eprint={1911.03588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2019bam,
      title={BAM! Born-Again Multi-Task Networks for Natural Language Understanding}, 
      author={Kevin Clark and Minh-Thang Luong and Urvashi Khandelwal and Christopher D. Manning and Quoc V. Le},
      year={2019},
      eprint={1907.04829},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bertscore,
  doi = {10.48550/ARXIV.1904.09675},
  
  url = {https://arxiv.org/abs/1904.09675},
  
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERTScore: Evaluating Text Generation with BERT},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{tang2019distilling,
      title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks}, 
      author={Raphael Tang and Yao Lu and Linqing Liu and Lili Mou and Olga Vechtomova and Jimmy Lin},
      year={2019},
      eprint={1903.12136},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kaliamoorthi2021distilling,
      title={Distilling Large Language Models into Tiny and Effective Students using pQRNN}, 
      author={Prabhu Kaliamoorthi and Aditya Siddhant and Edward Li and Melvin Johnson},
      year={2021},
      eprint={2101.08890},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sun2020mobilebert,
      title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices}, 
      author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
      year={2020},
      eprint={2004.02984},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2020minilm,
      title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}, 
      author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
      year={2020},
      eprint={2002.10957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jin2019knowledge,
      title={Knowledge Distillation via Route Constrained Optimization}, 
      author={Xiao Jin and Baoyun Peng and Yichao Wu and Yu Liu and Jiaheng Liu and Ding Liang and Junjie Yan and Xiaolin Hu},
      year={2019},
      eprint={1904.09149},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{khanuja2021mergedistill,
      title={MergeDistill: Merging Pre-trained Language Models using Distillation}, 
      author={Simran Khanuja and Melvin Johnson and Partha Talukdar},
      year={2021},
      eprint={2106.02834},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{CamemBERT2020,
   title={CamemBERT: a Tasty French Language Model},
   url={http://dx.doi.org/10.18653/v1/2020.acl-main.645},
   DOI={10.18653/v1/2020.acl-main.645},
   journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Martin, Louis and Muller, Benjamin and Ortiz Suárez, Pedro Javier and Dupont, Yoann and Romary, Laurent and de la Clergerie, Éric and Seddah, Djamé and Sagot, Benoît},
   year={2020}
}


@misc{k2020crosslingual,
      title={Cross-Lingual Ability of Multilingual BERT: An Empirical Study}, 
      author={Karthikeyan K and Zihan Wang and Stephen Mayhew and Dan Roth},
      year={2020},
      eprint={1912.07840},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{glavas2019properly,
      title={How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions}, 
      author={Goran Glavas and Robert Litschko and Sebastian Ruder and Ivan Vulic},
      year={2019},
      eprint={1902.00508},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lample2019crosslingual,
      title={Cross-lingual Language Model Pretraining}, 
      author={Guillaume Lample and Alexis Conneau},
      year={2019},
      eprint={1901.07291},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{doi:10.1177/107769905303000401,
author = {Wilson L. Taylor},
title ={“Cloze Procedure”: A New Tool for Measuring Readability},
journal = {Journalism Quarterly},
volume = {30},
number = {4},
pages = {415-433},
year = {1953},
doi = {10.1177/107769905303000401},

URL = { 
        https://doi.org/10.1177/107769905303000401
    
},
eprint = { 
        https://doi.org/10.1177/107769905303000401
    
}
,
    abstract = { Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas. }
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mccann2018learned,
      title={Learned in Translation: Contextualized Word Vectors}, 
      author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1708.00107},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dai2015semisupervised,
      title={Semi-supervised Sequence Learning}, 
      author={Andrew M. Dai and Quoc V. Le},
      year={2015},
      eprint={1511.01432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{soltanolkotabi2018theoretical,
      title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks}, 
      author={Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee},
      year={2018},
      eprint={1707.04926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{du2018power,
      title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation}, 
      author={Simon S. Du and Jason D. Lee},
      year={2018},
      eprint={1803.01206},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lauscher-etal-2020-zero,
    title = "From Zero to Hero: {O}n the Limitations of Zero-Shot Language Transfer with Multilingual {T}ransformers",
    author = "Lauscher, Anne  and
      Ravishankar, Vinit  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.363",
    doi = "10.18653/v1/2020.emnlp-main.363",
    pages = "4483--4499",
    abstract = "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
}

@misc{magueresse2020lowresource,
      title={Low-resource Languages: A Review of Past Work and Future Challenges}, 
      author={Alexandre Magueresse and Vincent Carles and Evan Heetderks},
      year={2020},
      eprint={2006.07264},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

{% bibliography --cited %}